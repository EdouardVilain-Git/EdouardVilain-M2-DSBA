{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decision_tree_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nddo0QGHZ02n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionBinaryTree():\n",
        "\n",
        "    def __init__(self, label):\n",
        "        \"\"\"\n",
        "        Build a binary tree class adapted to the Decision Tree architecture using the Node class implemented above.\n",
        "        \"\"\"\n",
        "        self.label = label\n",
        "        self.split_feature = None\n",
        "        self.split_value = None\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.depth = 0\n",
        "\n",
        "    def get_node(self, path):\n",
        "        \"\"\"\n",
        "        Runs through the tree given input path and returns corresponding node.\n",
        "        \"\"\"\n",
        "        node = self\n",
        "        for dir_ in path:\n",
        "            if dir_ == \"left\":\n",
        "                node = node.left_child\n",
        "            elif dir_ == \"right\":\n",
        "                node = node.right_child\n",
        "        return node\n",
        "\n",
        "    def add_split(self, path, split_feature, split_value, left_label, right_label):\n",
        "        \"\"\"\n",
        "        Adds a split at the given path and using the given information.\n",
        "        \"\"\"\n",
        "        node = self.get_node(path)\n",
        "        node.split_feature = split_feature\n",
        "        node.split_value = split_value\n",
        "        node.left_child = DecisionBinaryTree(left_label)\n",
        "        node.right_child = DecisionBinaryTree(right_label)\n",
        "        # Increment tree depth if path is as long as depth\n",
        "        if len(path) >= self.depth:\n",
        "            self.depth += 1"
      ],
      "metadata": {
        "id": "6x_Flv0tebae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gini(y,split_index):\n",
        "  left_split, right_split = y[:split_index], y[split_index:]\n",
        "\n",
        "  # Compute left and right gini index\n",
        "  left_gini = 1 - np.sum(np.square(np.unique(left_split, return_counts=True)[1] / left_split.shape[0]))\n",
        "  right_gini = 1 - np.sum(np.square(np.unique(right_split, return_counts=True)[1] / right_split.shape[0]))\n",
        "  \n",
        "  # Return weighted means of gini index\n",
        "  return (left_split.shape[0] * left_gini / X.shape[0]) + (right_split.shape[0] * right_gini / X.shape[0])"
      ],
      "metadata": {
        "id": "d7yvnQawTjNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def crossentropy(y,split_index):\n",
        "  left_split, right_split = y[:split_index], y[split_index:]\n",
        "\n",
        "  # Compute left and right crossentropy\n",
        "  left_probs, right_probs = np.unique(left_split, return_counts=True)[1] / left_split.shape[0], np.unique(right_split, return_counts=True)[1] / right_split.shape[0]\n",
        "  left_crossentropy = - np.sum(left_probs * np.log2(left_probs))\n",
        "  right_crossentropy = - np.sum(right_probs * np.log2(right_probs))\n",
        "\n",
        "  return (left_split.shape[0] * left_crossentropy / X.shape[0]) + (right_split.shape[0] * right_crossentropy / X.shape[0])"
      ],
      "metadata": {
        "id": "FlIP55ZWTk-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_split(indices,X,y,criterion,min_samples_split):\n",
        "    \"\"\"\n",
        "    Returns best split value along with indexes of the left and right leaf it creates.\n",
        "    Args:\n",
        "        indices (np.array): sample indices\n",
        "        X (np.array): feature vector\n",
        "        y (np.array): label vector\n",
        "        feature (int): index of split feature\n",
        "        criterion (String): either \"gini\" or \"crossentropy\", split metric\n",
        "        min_samples_split (int): number of minimum samples that must be contained in each leaf of the split\n",
        "    \"\"\"\n",
        "    # Sort indices \n",
        "    sort_indices = np.argsort(X)\n",
        "    indices_sorted, X_sorted, y_sorted = indices[sort_indices], X[sort_indices], y[sort_indices]\n",
        "\n",
        "    # Find best split\n",
        "    best_split_index, best_criterion_value = None, None\n",
        "    for i in range(min_samples_split-1,X_sorted.shape[0]-min_samples_split): # Only run through possible splits with regards to min_samples_split\n",
        "        if criterion == \"gini\":\n",
        "            criterion_value = gini(y_sorted, i)\n",
        "        elif criterion == \"crossentropy\":\n",
        "            criterion_value = crossentropy(y_sorted, i)\n",
        "        else:\n",
        "            raise ValueError(\"\"\"Wrong criterion! Must be in [\"gini\",\"crossentropy\"]\"\"\")\n",
        "\n",
        "        # Update best split\n",
        "        if best_criterion_value is None:\n",
        "            best_criterion_value, best_split_index = criterion_value, i\n",
        "        else:\n",
        "            if best_criterion_value > criterion_value:\n",
        "                best_criterion_value, best_split_index = criterion_value, i\n",
        "    \n",
        "    # Return best criterion value, split value and indexes\n",
        "    return best_criterion_value, X_sorted[best_split_index], indices_sorted[:best_split_index+1], indices_sorted[best_split_index+1:]"
      ],
      "metadata": {
        "id": "Y17A9C0ABsTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree_Classification():\n",
        "    \"\"\"\n",
        "    Decision Tree Model. Will benefit from the same structure as the sklearn implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, criterion=\"gini\", max_depth=None, max_features=None, min_samples_split=2, random_state=None):\n",
        "        \"\"\"\n",
        "        Attributes:\n",
        "            criterion (string): split criterion, either \"gini\" or \"crossentropy\"\n",
        "            max_depth (int): maximum tree depth\n",
        "            max_features (int): number of features to consider when looking for a best split\n",
        "            min_sample_split (int): minimum number of instances in each set in order to split\n",
        "            random_state (int): random seed\n",
        "            depth (int): tree depth\n",
        "            splits (dict): dictionary containing split conditions, each leaf is described by the path taken to the leaf (list of \"left\" and \"right\")\n",
        "            subsets (dict): maps each leaf to the subset of samples it describes\n",
        "        \"\"\"\n",
        "        # Set input attribute values\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features    \n",
        "        self.min_samples_split = 2\n",
        "        self.random_state = random_state\n",
        "\n",
        "        # Set initial tree state attributes\n",
        "        self.depth = 0\n",
        "        self.splits = DecisionBinaryTree(None)\n",
        "        self.subsets = {}\n",
        "\n",
        "    def fit(self, X, y, verbose=True):\n",
        "        \"\"\"\n",
        "        Fit model with input data.\n",
        "        Args:\n",
        "            X (np.array): feature array of dimension (nb_samples,nb_features)\n",
        "            y (np.array): label array of dimension (nb_samples)\n",
        "        \"\"\"\n",
        "        start = time.time()\n",
        "        # Set initial state as one complete set of all data\n",
        "        self.subsets[\"[]\"] = np.arange(X.shape[0])\n",
        "        # Set random seed, if any is given\n",
        "        if not self.random_state is None:\n",
        "            np.random.seed(self.random_state)\n",
        "\n",
        "        # Split while a break condition has not been met\n",
        "        break_condition = False\n",
        "        while not break_condition:\n",
        "            leafs = list(self.subsets.items()) # Get leafs and their corresponding train samples\n",
        "            # Set list of features on which split is applied\n",
        "            if self.max_features is None:\n",
        "                split_features = np.arange(X.shape[1]) # If no feature constraint is given, all features must be considered for split\n",
        "            else:\n",
        "                nb_features = min(self.max_features,X.shape[1]) # Take the min between the total number of features and the max_features value\n",
        "                split_features = np.random.choice(np.arange(X.shape[1]), nb_features) # Consider a random sample of nb_features features on which to apply the split\n",
        "            best_split_criterion  = None # Contains best split attributes\n",
        "\n",
        "            # Run through each leaf and find best split\n",
        "            for leaf in leafs:\n",
        "                leaf_path, leaf_set = leaf\n",
        "                # Check if current leaf can be extended\n",
        "                if (not self.max_depth is None) and (len(eval(leaf_path)) == self.max_depth): # If maximum tree depth is met\n",
        "                    continue\n",
        "\n",
        "                # Check if leaf has enough elements to be split\n",
        "                if len(leaf_set) < 2 * self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                X_subset, y_subset = X[leaf_set,:], y[leaf_set]\n",
        "                \n",
        "                best_leaf_criterion = None # Instantiate best leaf split\n",
        "                for feature in split_features:\n",
        "                    X_subset_feat = X_subset[:,feature]\n",
        "                    best_feat_criterion, feat_split_value, left_indices_feat, right_indices_feat = get_best_split(leaf_set, X_subset_feat, y_subset, self.criterion, self.min_samples_split)\n",
        "\n",
        "                    # Check if it is best split in leaf\n",
        "                    if (best_leaf_criterion) is None or (best_leaf_criterion > best_feat_criterion):\n",
        "                        leaf_split_feature, best_leaf_criterion, leaf_split_value, left_indices_leaf, right_indices_leaf = feature, best_feat_criterion, feat_split_value, left_indices_feat, right_indices_feat\n",
        "\n",
        "                # Check if leaf has the best split\n",
        "                if (best_split_criterion is None) or (best_split_criterion > best_leaf_criterion):\n",
        "                    best_leaf, split_feature, best_split_criterion, split_value, left_indices, right_indices = leaf_path, leaf_split_feature, best_leaf_criterion, leaf_split_value, left_indices_leaf, right_indices_leaf\n",
        "\n",
        "            # Best split has been found, check if valid. If so, create new split, else break\n",
        "            if best_split_criterion is None: # If no split criterion has been found, break (e.g. all nodes have small amounts of data)\n",
        "                break_condition = True\n",
        "            else:\n",
        "                if verbose:\n",
        "                    print(\"Split of leaf:\",best_leaf)\n",
        "                    print(\"   | On feature:\",split_feature)\n",
        "                    print(\"   | At value:\", split_value)\n",
        "                \n",
        "                # Update splits and subsets attributes\n",
        "                best_leaf_path = eval(best_leaf) # Get path to best leaf as list\n",
        "\n",
        "                self.subsets.pop(best_leaf) # Remove old leaf\n",
        "                left_key, right_key = str(best_leaf_path + [\"left\"]), str(best_leaf_path + [\"right\"])\n",
        "                self.subsets[left_key] = left_indices\n",
        "                self.subsets[right_key] = right_indices\n",
        "\n",
        "                # Compute label of each split\n",
        "                left_labels, right_labels = y[left_indices], y[right_indices]\n",
        "                left_vals, left_occs = np.unique(left_labels, return_counts=True)\n",
        "                right_vals, right_occs = np.unique(right_labels, return_counts=True)\n",
        "                left_label, right_label = left_vals[np.argmax(left_occs)], right_vals[np.argmax(right_occs)]\n",
        "\n",
        "                self.splits.add_split(best_leaf_path, split_feature, split_value, left_label, right_label)\n",
        "\n",
        "        stop = time.time()\n",
        "        print(\"Training time:\",round(stop-start,2),\"s\")\n",
        "\n",
        "\n",
        "    def unique_pred(self, X_row):\n",
        "        \"\"\"\n",
        "        Make a prediction on input row. Run through the binary split tree until a leaf is encountered.\n",
        "        \"\"\"\n",
        "        # Start from top of the split tree and run down to a leaf according to the split conditions\n",
        "        node = self.splits\n",
        "        while not node.left_child is None: # While node is not terminal\n",
        "            split_feature, split_value = node.split_feature, node.split_value\n",
        "            if X_row[split_feature] <= split_value: # Go to the left child\n",
        "                node = node.left_child\n",
        "            else:\n",
        "                node = node.right_child\n",
        "        return node.label\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.apply_along_axis(self.unique_pred, 1, X)\n",
        "\n",
        "    def accuracy_score(self, X, y):\n",
        "        y_pred = self.predict(X)\n",
        "        return accuracy_score(y,y_pred)"
      ],
      "metadata": {
        "id": "w313rjybaLsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare Results with SK-learn"
      ],
      "metadata": {
        "id": "CEGg01bEjUw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X,y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "1fakiWrPjfDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first train an sklearn Decision Tree Classifier with a maximum depth of 4:"
      ],
      "metadata": {
        "id": "iZW2-v6Xj2_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt_sklearn = DecisionTreeClassifier(max_depth=4)\n",
        "\n",
        "# Train\n",
        "start = time.time()\n",
        "dt_sklearn.fit(X_train,y_train)\n",
        "stop = time.time()\n",
        "print(\"Training Time:\",stop-start,\"s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFHOqIOcj_Zt",
        "outputId": "ccce44c1-9926-4640-aa15-c2faa6175077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Time: 0.012940168380737305 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make test prediction\n",
        "print(\"Test Accuracy:\",dt_sklearn.score(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n60yx1MEj_7a",
        "outputId": "5e4e61bd-d9e8-4b90-9c1c-1491b368283c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9385964912280702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now train our own model on the same dataset, with the same parameters:"
      ],
      "metadata": {
        "id": "RcTUIihekjZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTree_Classification(max_depth = 4)\n",
        "dt.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPbZGA9Pj_9U",
        "outputId": "f406c26e-d1e4-4e46-e5ed-ee4cadf4fc7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split of leaf: []\n",
            "   | On feature: 7\n",
            "   | At value: 0.05182\n",
            "Split of leaf: ['left']\n",
            "   | On feature: 20\n",
            "   | At value: 16.82\n",
            "Split of leaf: ['left', 'left']\n",
            "   | On feature: 10\n",
            "   | At value: 0.645\n",
            "Split of leaf: ['left', 'left', 'left']\n",
            "   | On feature: 13\n",
            "   | At value: 41.0\n",
            "Split of leaf: ['right']\n",
            "   | On feature: 27\n",
            "   | At value: 0.1466\n",
            "Split of leaf: ['right', 'right']\n",
            "   | On feature: 16\n",
            "   | At value: 0.1278\n",
            "Split of leaf: ['right', 'right', 'left']\n",
            "   | On feature: 0\n",
            "   | At value: 11.42\n",
            "Split of leaf: ['left', 'right']\n",
            "   | On feature: 1\n",
            "   | At value: 16.68\n",
            "Split of leaf: ['left', 'right', 'left']\n",
            "   | On feature: 11\n",
            "   | At value: 0.3628\n",
            "Split of leaf: ['left', 'right', 'right']\n",
            "   | On feature: 11\n",
            "   | At value: 1.439\n",
            "Split of leaf: ['right', 'left']\n",
            "   | On feature: 22\n",
            "   | At value: 116.2\n",
            "Split of leaf: ['right', 'left', 'left']\n",
            "   | On feature: 1\n",
            "   | At value: 21.35\n",
            "Split of leaf: ['right', 'left', 'right']\n",
            "   | On feature: 0\n",
            "   | At value: 16.6\n",
            "Training time: 9.08 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy:\",dt.accuracy_score(X_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDMyq2WuijoF",
        "outputId": "18e27b34-8214-4296-e14a-b94770710bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}